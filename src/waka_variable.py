"""waka_variable.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oVURf2H7T71bBVebr9zSFwB_QrH7U1mh

# Dependencies
"""

import json
import numpy as np
import pandas as pd
from collections import Counter, OrderedDict, defaultdict
from heapq import nlargest
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import ClusterWarning
from warnings import simplefilter
simplefilter("ignore", ClusterWarning)

# load data
with open('../data/id2lemma.json') as d, \
     open("../cache/context_words.txt") as c:
    id2lemma = json.load(d)
    feature = c.read().split("\n")

hd = pd.read_csv('../data/parsed_hd.csv')

stop_lst = ['03', '04', '05', '07', '08', '09', '16']
lemma_lst = [x for x in id2lemma.keys() if x.split('-')[1] not in stop_lst]
hd['cleaned'] = hd.source.map(
    lambda x: [x for x in x.split(',') if x in lemma_lst])

# 总处理token数
print('numbers of tokens: %s' % sum(hd.source.str.split(',').map(len)))
# clean后type数
print('numbers of types: %s' % len(lemma_lst))
"""# Functions

## Co-occurrence frequency matrix
"""


def co_oc_mat(texts, window_size):
    count_d = defaultdict(int)
    types = set()
    for text in texts:
        for i in range(len(text)):
            token = text[i]
            types.add(token)
            next_token = text[i + 1:i + 1 + window_size]
            for t in next_token:
                key = tuple(sorted([t, token]))
                count_d[key] += 1

    types = sorted(types)
    mat = pd.DataFrame(data=np.zeros((len(feature), len(types)),
                                     dtype=np.int16),
                       index=feature,
                       columns=types)

    for key, value in count_d.items():
        mat.at[key[0], key[1]] = value
        mat.at[key[1], key[0]] = value

    return mat


def ppmi(freq_mat):
    '''Obtain ppmi matrix.'''
    df = freq_mat.copy()
    col_totals = df.sum(axis=0)
    total = col_totals.sum()
    row_totals = df.sum(axis=1)
    expected = np.outer(row_totals, col_totals) / total
    df = df / expected
    with np.errstate(divide='ignore'):
        df = np.log2(df)
    df[np.isinf(df)] = 0.0  # log(0) = 0
    df[np.isnan(df)] = 0.0
    df[df < 0] = 0.0

    return df


def cosin_sim(v_i, v_j):
    return np.dot(v_i, v_j) / (np.linalg.norm(v_i) * np.linalg.norm(v_j))


def sim_mat(ppmi_mat, target_lemma_lst):
    '''Obtain similarity matrix.'''
    sim_d = defaultdict()
    t_set = sorted(set(target_lemma_lst))
    for i in range(len(target_lemma_lst)):
        t_i = target_lemma_lst[i]
        next_t = target_lemma_lst[i:]
        for t_j in next_t:
            key = tuple(sorted([t_i, t_j]))
            v_t_i = np.array(ppmi_mat[t_i])
            v_t_j = np.array(ppmi_mat[t_j])
            sim_d[key] = cosin_sim(v_t_i, v_t_j)
    mat = pd.DataFrame(data=np.zeros((len(t_set), len(t_set)),
                                     dtype=np.float64),
                       index=t_set,
                       columns=t_set)
    for key, value in sim_d.items():
        mat.at[key[0], key[1]] = value
        mat.at[key[1], key[0]] = value

    return mat


def cluster(cos_mat, l, k, alpha):
    top_n_l = cos_mat.nlargest(k, l).index.tolist()
    if len(top_n_l) == 1:
        res = [tuple(top_n_l)]
    else:
        sub_CosM = np.array(cos_mat.loc[top_n_l, top_n_l])
        clusters = AgglomerativeClustering(n_clusters=None,
                                           distance_threshold=alpha,
                                           affinity="cosine",
                                           linkage="average").fit(sub_CosM)
        labels = list(clusters.labels_)
        labeled_lemma = list(zip(top_n_l, labels))
        C = {}
        for label in set(labels):
            C[label] = tuple()
            for lemma in labeled_lemma:
                if lemma[1] == label:
                    C[label] += (lemma[0], )
                    C[label] = tuple(sorted(C[label]))
        res = list(C.values())

    return res


def committee(target_lemma_lst, cos_mat, k, alpha):
    C = []
    for l in target_lemma_lst:
        C += cluster(cos_mat.loc[target_lemma_lst, target_lemma_lst], l, k,
                     alpha)
    return C


def avg_sim(cos_mat, c):
    s = 0
    times = 0
    for i in range(len(c)):
        if len(c) == 1:
            avg_s = 1
        else:
            c_i = c[i]
            next_c = c[i + 1:]
            for c_j in next_c:
                times += 1
                s += cos_mat.loc[c_i, c_j]
            # print(s, times)
            if times != 0:
                avg_s = s / times

    return avg_s


def filer_sort(C, cos_mat):
    # remove dublicate c
    C = list(set(C))
    rm_lst = set()
    for i in range(len(C)):
        c_i = C[i]
        next_c = C[:i] + C[i + 1:]
        for c_j in next_c:
            if len(c_i) == 1:  # remove singleton included in other larger c
                # print(c_i, c_j)
                if set(c_i).issubset(c_j):
                    rm_lst.add(c_i)
            else:  # remove larger c if c included other smaller c
                if set(c_i).issubset(c_j):
                    rm_lst.add(c_j)

    for c in rm_lst:
        C.remove(c)

    C_s = {}
    for c in C:
        C_s[c] = avg_sim(cos_mat, c)

    res = list(zip(C_s.keys(), C_s.values()))
    # sort
    res = sorted(res, key=lambda x: x[1])

    return res


# merging loop
def merge_1(C, freq_mat, feature=feature):
    c_vec_dic = {}
    for c in C:
        avg_freq_vec = freq_mat[list(c[0])].mean(axis=1)
        c_vec_dic[str(c[0])[1:-1]] = list(avg_freq_vec)

    CFreqM = pd.DataFrame(c_vec_dic)
    CFreqM.index = freq_mat.index
    CppmiM = ppmi(CFreqM)
    CppmiM = CppmiM.loc[CppmiM.index.isin(feature)]
    ComM = sim_mat(CppmiM, list(CppmiM.columns))
    return CFreqM, CppmiM, ComM


def str2tup(c):
    c = c.replace('\'', '').replace(' ', '').split(',')
    if '' in c:
        c.remove('')
    return tuple(c)


def merge_2(committee_mat, cos_mat, beta):
    new_C = []
    merged_lst = []
    c_lst = committee_mat.columns
    n_c = len(c_lst)
    for i in range(len(c_lst)):
        target_c = c_lst[i]
        neighbor_c_lst = committee_mat.nlargest(n_c, target_c).index.tolist()
        n = 0
        neighbor_c = None
        while (neighbor_c in merged_lst) | (neighbor_c == None):
            n += 1
            if n < len(committee_mat):
                neighbor_c = neighbor_c_lst[-1]
            else:
                neighbor_c = target_c
        sim = committee_mat.loc[target_c, neighbor_c]
        if (sim >= beta) & (neighbor_c != target_c):
            merged_lst.append(neighbor_c)
            # print(target_c, '-', neighbor_c)
            target_c = str2tup(target_c)
            neighbor_c = str2tup(neighbor_c)
            new_c = tuple(target_c + neighbor_c)
            new_c = tuple(sorted(set(new_c)))
            # print(new_c)
        else:
            target_c = str2tup(target_c)
            new_c = target_c

        if new_c not in new_C:
            # print(new_c)
            new_C.append(new_c)

    # print(new_C)

    new_C_s = {}
    for c in new_C:
        new_C_s[c] = avg_sim(cos_mat, c)

    new_C_s = list(zip(new_C_s.keys(), new_C_s.values()))
    new_C_s = sorted(new_C_s, key=lambda x: x[1])

    return new_C_s


def residuals(target_lemma_lst, ppmi_mat, c_ppmi_mat, gamma):
    R = []
    for l in target_lemma_lst:
        v_l = np.array(ppmi_mat[l])
        sim = 0
        n = 0
        c = ''
        while (n < len(c_ppmi_mat.columns)) & (sim < gamma):
            c = c_ppmi_mat.columns[n]
            v_c = np.array(c_ppmi_mat[c])
            sim = cosin_sim(v_l, v_c)
            # print(l, c, sim)
            n += 1
        if sim < gamma:
            # print(l, c, sim)
            R.append(l)
        else:
            # print(c, l, sim)
            pass
    return R


"""# Loop"""


def search(k, alpha, beta, gamma, window_size, TOP):

    token_lst = []
    for poem in hd.cleaned:
        token_lst += poem

    lemma_freq_dic = dict(OrderedDict(Counter(token_lst).most_common()))
    target_lemma_lst = nlargest(TOP, lemma_freq_dic, key=lemma_freq_dic.get)

    # initial matrice
    FreqM = co_oc_mat(hd.cleaned, window_size)
    ppmiM = ppmi(FreqM)
    ppmiM = ppmiM.loc[ppmiM.index.isin(feature), target_lemma_lst]
    CosM = sim_mat(ppmiM, target_lemma_lst)

    # loop
    R = target_lemma_lst
    n = 0
    C_final = []
    while R != []:
        n += 1
        # initial clustered committee
        C = committee(R, CosM, k, alpha)
        # filter and sort
        C = filer_sort(C, CosM)
        # merge part1
        CFreqM, CppmiM, ComM = merge_1(C, FreqM)
        # merge part2
        new_C_s = merge_2(ComM, CosM, beta)
        # repeat merge1
        C_new = sorted(new_C_s, key=lambda x: x[1])
        print(C_new)
        CFreqM_new, CppmiM_new, ComM_new = merge_1(C_new, FreqM)
        R = residuals(R, ppmiM, CppmiM_new, gamma)
        for c_new in C_new:
            c_new_checked = True
            for c_old in C_final:
                if set(c_old[0]).issubset(c_new[0]):
                    C_final.remove(c_old)
                if set(c_new[0]).issubset(c_old[0]):
                    c_new_checked = False
            if (c_new not in C_final) & (c_new_checked):
                C_final.append(c_new)
        print(n, R)

    C_final = sorted(C_final, key=lambda x: x[1])

    # save result
    summary = pd.DataFrame()
    summary['bg_id'] = list(zip(*C_final))[0]
    summary['lemma'] = summary.bg_id.map(
        lambda x: tuple([id2lemma[l][0] for l in x]))
    summary['reading'] = summary.bg_id.map(
        lambda x: tuple([id2lemma[l][1] for l in x]))
    summary['average_similarity'] = list(zip(*C_final))[1]
    summary.to_csv(
        '../res/k-{};alpha-{};beta-{};gamma-{};window_size-{};TOP-{}.csv'.
        format(k, alpha, beta, gamma, window_size, TOP),
        index=False)

    return summary


# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=2, TOP=500)

# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=2, TOP=1000)

# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=3, TOP=500)

# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=3, TOP=1000)

# search(k=50, alpha=0.5, beta=0.4, gamma=0.2, window_size=2, TOP=1000)

# search(k=50, alpha=0.1, beta=0.4, gamma=0.6, window_size=2, TOP=1000)

# search(k=50, alpha=0.3, beta=0.8, gamma=0.6, window_size=2, TOP=1000)

# search(k=50, alpha=0.3, beta=0.8, gamma=0.4, window_size=2, TOP=1000)

# search(k=50, alpha=0.3, beta=0.8, gamma=0.4, window_size=3, TOP=1000)

# search(k=100, alpha=0.5, beta=0.8, gamma=0.6, window_size=4, TOP=1000)

search(k=100, alpha=0.5, beta=0.8, gamma=0.6, window_size=2, TOP=2000)
