"""waka_variable.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oVURf2H7T71bBVebr9zSFwB_QrH7U1mh

# Dependencies
"""

import json
import numpy as np
import pandas as pd
from collections import Counter, OrderedDict, defaultdict
from heapq import nlargest
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import ClusterWarning
from warnings import simplefilter
simplefilter("ignore", ClusterWarning)

# load data
with open('../data/id2lemma.json', "w") as d, \
     open("../cache/context_words.txt", "w") as c:
    id2lemma = json.load(d)
    feature = c.read().split("\n")

hd = pd.read_csv('../data/parsed_hd.csv')

stop_lst = ['03', '04', '05', '07', '08', '09', '16']
lemma_lst = [x for x in id2lemma.keys() if x.split('-')[1] not in stop_lst]
hd['cleaned'] = hd.source.map(
    lambda x: [x for x in x.split(',') if x in lemma_lst])

gamma = 0.6
k = 50
alpha = 0.2
beta = 0.8
gamma = 0.6
TOP = 500

token_lst = []
for poem in hd.cleaned:
    token_lst += poem

lemma_freq_dic = dict(OrderedDict(Counter(token_lst).most_common()))
target_lemma_lst = nlargest(TOP, lemma_freq_dic, key=lemma_freq_dic.get)
R = target_lemma_lst

# 总处理token数
print('numbers of tokens: %s' % sum(hd.source.str.split(',').map(len)))
# clean后token数
# print('numbers of tokens: %s' % sum(hd.cleaned.map(len)))
# clean后type数
print('numbers of types: %s' % len(lemma_lst))
"""# Functions

## Co-occurrence frequency matrix
"""


def co_oc_mat(texts, window_size):
    """Obtain co-occcurrence count matrix."""
    count_d = defaultdict(int)
    token_set = set()
    for text in texts:
        for i in range(len(text)):
            token = text[i]
            if token in lemma_lst:
                token_set.add(token)
                next_token = text[i + 1:i + 1 + window_size]
                for t in next_token:
                    if t in feature:
                        key = tuple(sorted([t, token]))
                        count_d[key] += 1
    token_set = sorted(token_set)
    mat = pd.DataFrame(data=np.zeros((len(token_set), len(token_set)),
                                     dtype=np.int16),
                       index=token_set,
                       columns=token_set)
    for key, value in count_d.items():
        mat.at[key[0], key[1]] = value
        mat.at[key[1], key[0]] = value

    return mat


"""## PPMI matrix"""


def ppmi(freq_mat):
    '''Obtain ppmi matrix.'''
    df = freq_mat.copy()
    col_totals = df.sum(axis=0)
    total = col_totals.sum()
    row_totals = df.sum(axis=1)
    expected = np.outer(row_totals, col_totals) / total
    df = df / expected
    # Silence distracting warnings about log(0):
    with np.errstate(divide='ignore'):
        df = np.log(df)
    df[np.isinf(df)] = 0.0  # log(0) = 0
    df[np.isnan(df)] = 0.0
    df[df < 0] = 0.0

    return df


"""## Cosine similarity matrix"""


def cosin_sim(v_i, v_j):
    return np.dot(v_i, v_j) / (np.linalg.norm(v_i) * np.linalg.norm(v_j))


def sim_mat(ppmi_mat, target_lemma_lst):
    '''Obtain similarity matrix.'''
    sim_d = defaultdict()
    t_set = sorted(set(target_lemma_lst))
    for i in range(len(target_lemma_lst)):
        t_i = target_lemma_lst[i]
        next_t = target_lemma_lst[i:]
        for t_j in next_t:
            key = tuple(sorted([t_i, t_j]))
            v_t_i = np.array(ppmi_mat[t_i])
            v_t_j = np.array(ppmi_mat[t_j])
            sim_d[key] = cosin_sim(v_t_i, v_t_j)
    mat = pd.DataFrame(data=np.zeros((len(t_set), len(t_set)),
                                     dtype=np.float64),
                       index=t_set,
                       columns=t_set)
    for key, value in sim_d.items():
        mat.at[key[0], key[1]] = value
        mat.at[key[1], key[0]] = value

    return mat


def cluster(cos_mat, l, k, alpha):
    top_n_l = cos_mat.nlargest(k, l).index.tolist()
    if len(top_n_l) == 1:
        res = [tuple(top_n_l)]
    else:
        sub_CosM = np.array(cos_mat.loc[top_n_l, top_n_l])
        clusters = AgglomerativeClustering(n_clusters=None,
                                           distance_threshold=alpha,
                                           affinity="cosine",
                                           linkage="average").fit(sub_CosM)
        labels = list(clusters.labels_)
        labeled_lemma = list(zip(top_n_l, labels))
        C = {}
        for label in set(labels):
            C[label] = tuple()
            for lemma in labeled_lemma:
                if lemma[1] == label:
                    C[label] += (lemma[0], )
                    C[label] = tuple(sorted(C[label]))
        res = list(C.values())
    # print(C)

    return res


def committee(target_lemma_lst, cos_mat):
    C = []
    for l in target_lemma_lst:
        C += cluster(cos_mat.loc[target_lemma_lst, target_lemma_lst], l, k,
                     alpha)
    return C


"""## Committee filter and sort"""


def avg_sim(cos_mat, c):
    s = 0
    times = 0
    for i in range(len(c)):
        if len(c) == 1:
            avg_s = 1
        else:
            c_i = c[i]
            next_c = c[i + 1:]
            for c_j in next_c:
                times += 1
                s += cos_mat.loc[c_i, c_j]
            # print(s, times)
            if times != 0:
                avg_s = s / times

    return avg_s


def filer_sort(C, cos_mat):
    # remove dublicate c
    C = list(set(C))
    rm_lst = set()
    for i in range(len(C)):
        c_i = C[i]
        next_c = C[:i] + C[i + 1:]
        for c_j in next_c:
            if len(c_i) == 1:  # remove singleton included in other larger c
                # print(c_i, c_j)
                if set(c_i).issubset(c_j):
                    rm_lst.add(c_i)
            else:  # remove larger c if c included other smaller c
                if set(c_i).issubset(c_j):
                    rm_lst.add(c_j)

    for c in rm_lst:
        C.remove(c)

    C_s = {}
    for c in C:
        C_s[c] = avg_sim(cos_mat, c)

    res = list(zip(C_s.keys(), C_s.values()))
    # sort
    res = sorted(res, key=lambda x: x[1])

    return res


"""## Merge 1"""


# merging loop
def merge_1(C, freq_mat):
    C_with_vec = []
    for c in C:
        avg_freq_vec = np.array(freq_mat[list(c[0])].mean(axis=1))
        c = c + (avg_freq_vec, )
        C_with_vec.append(c)

    c_vec_dic = {}
    for c in C_with_vec:
        c_vec_dic[str(c[0])[1:-1]] = list(c[-1])

    CFreqM = pd.DataFrame(c_vec_dic)
    CppmiM = ppmi(CFreqM)
    ComM = sim_mat(CppmiM, list(CppmiM.columns))
    return CFreqM, CppmiM, ComM


"""## Merge 2"""


def str2tup(c):
    c = c.replace('\'', '').replace(' ', '').split(',')
    if '' in c:
        c.remove('')
    return tuple(c)


def merge_2(committee_mat, cos_mat):
    new_C = []
    merged_lst = []
    c_lst = committee_mat.columns
    n_c = len(c_lst)
    for i in range(len(c_lst)):
        # print(i)
        target_c = c_lst[i]
        neighbor_c_lst = committee_mat.nlargest(n_c, target_c).index.tolist()
        n = 0
        neighbor_c = None
        while (neighbor_c in merged_lst) | (neighbor_c == None):
            n += 1
            if n < len(committee_mat):
                neighbor_c = neighbor_c_lst[-1]
            else:
                neighbor_c = target_c
        # print(neighbor_c)
        sim = committee_mat.loc[target_c, neighbor_c]
        if (sim >= beta) & (neighbor_c != target_c):
            merged_lst.append(neighbor_c)
            # print(target_c, '-', neighbor_c)
            target_c = str2tup(target_c)
            neighbor_c = str2tup(neighbor_c)
            new_c = tuple(target_c + neighbor_c)
            new_c = tuple(sorted(set(new_c)))
            # print(new_c)
        else:
            target_c = str2tup(target_c)
            new_c = target_c

        if new_c not in new_C:
            # print(new_c)
            new_C.append(new_c)

    # print(new_C)

    new_C_s = {}
    for c in new_C:
        new_C_s[c] = avg_sim(cos_mat, c)

    new_C_s = list(zip(new_C_s.keys(), new_C_s.values()))
    new_C_s = sorted(new_C_s, key=lambda x: x[1])

    return new_C_s


"""## Save residuals"""


def residuals(target_lemma_lst, ppmi_mat, c_ppmi_mat):
    R = []
    for l in target_lemma_lst:
        v_l = np.array(ppmi_mat[l])
        sim = 0
        n = 0
        c = ''
        while (n < len(c_ppmi_mat.columns)) & (sim < gamma):
            c = c_ppmi_mat.columns[n]
            v_c = np.array(c_ppmi_mat[c])
            sim = cosin_sim(v_l, v_c)
            # print(l, c, sim)
            n += 1
        if sim < gamma:
            # print(l, c, sim)
            R.append(l)
        else:
            # print(c, l, sim)
            pass
    return R


"""# Loop"""

k = 50
alpha = 0.5
beta = 0.8
gamma = 0.6
window_size = 2

TOP = 500

token_lst = []
for poem in hd.cleaned:
    token_lst += poem

lemma_freq_dic = dict(OrderedDict(Counter(token_lst).most_common()))
target_lemma_lst = nlargest(TOP, lemma_freq_dic, key=lemma_freq_dic.get)

# initial matrice
FreqM = co_oc_mat(hd.source.str.split(','), window_size)
ppmiM = ppmi(FreqM)
CosM = sim_mat(ppmiM, target_lemma_lst)


def search(k, alpha, beta, gamma, window_size, TOP):

    token_lst = []
    for poem in hd.cleaned:
        token_lst += poem

    lemma_freq_dic = dict(OrderedDict(Counter(token_lst).most_common()))
    target_lemma_lst = nlargest(TOP, lemma_freq_dic, key=lemma_freq_dic.get)

    # initial matrice
    FreqM = co_oc_mat(hd.source.str.split(','), window_size)
    ppmiM = ppmi(FreqM)
    CosM = sim_mat(ppmiM, target_lemma_lst)

    # loop
    R = target_lemma_lst
    n = 0
    C_final = []
    while R != []:
        n += 1
        # initial clustered committee
        C = committee(R, CosM)
        # filter and sort
        C = filer_sort(C, CosM)
        # merge part1
        CFreqM, CppmiM, ComM = merge_1(C, FreqM)
        # merge part2
        new_C_s = merge_2(ComM, CosM)
        # repeat merge1
        C_new = sorted(new_C_s, key=lambda x: x[1])
        print(C_new)
        CFreqM_new, CppmiM_new, ComM_new = merge_1(C_new, FreqM)
        R = residuals(R, ppmiM, CppmiM_new)
        for c_new in C_new:
            c_new_checked = True
            for c_old in C_final:
                if set(c_old[0]).issubset(c_new[0]):
                    C_final.remove(c_old)
                if set(c_new[0]).issubset(c_old[0]):
                    c_new_checked = False
            if (c_new not in C_final) & (c_new_checked):
                C_final.append(c_new)
        print(n, R)

    C_final = sorted(C_final, key=lambda x: x[1])

    # save result
    summary = pd.DataFrame()
    summary['bg_id'] = list(zip(*C_final))[0]
    summary['lemma'] = summary.bg_id.map(
        lambda x: tuple([id2lemma[l][0] for l in x]))
    summary['reading'] = summary.bg_id.map(
        lambda x: tuple([id2lemma[l][1] for l in x]))
    summary['average_similarity'] = list(zip(*C_final))[1]
    summary.to_csv(
        '../res/k-{};alpha-{};beta-{};gamma-{};window_size-{};TOP-{}.csv'.
        format(k, alpha, beta, gamma, window_size, TOP),
        index=False)

    return summary


# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=2, TOP=500)

# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=2, TOP=1000)

# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=3, TOP=500)

# search(k=50, alpha=0.5, beta=0.8, gamma=0.6, window_size=3, TOP=1000)

# search(k=50, alpha=0.5, beta=0.4, gamma=0.2, window_size=2, TOP=1000)

# search(k=50, alpha=0.1, beta=0.4, gamma=0.6, window_size=2, TOP=1000)

# search(k=50, alpha=0.3, beta=0.8, gamma=0.6, window_size=2, TOP=1000)

# search(k=50, alpha=0.3, beta=0.8, gamma=0.4, window_size=2, TOP=1000)

# search(k=50, alpha=0.3, beta=0.8, gamma=0.4, window_size=3, TOP=1000)

# search(k=100, alpha=0.5, beta=0.8, gamma=0.6, window_size=4, TOP=1000)

# search(k=100, alpha=0.5, beta=0.8, gamma=0.6, window_size=2, TOP=2000)
"""# Test 2"""

R = target_lemma_lst
n = 0
C_final = []
while R != []:
    n += 1
    # initial clustered committee
    C = committee(R)
    # filter and sort
    C = filer_sort(C)
    # merge part1
    CFreqM, CppmiM, ComM = merge_1(C, FreqM)
    # merge part2
    new_C_s = merge_2(ComM)
    # repeat merge1
    C_new = sorted(new_C_s, key=lambda x: x[1])
    print(C_new)
    CFreqM_new, CppmiM_new, ComM_new = merge_1(C_new, FreqM)
    R = residuals(R, CppmiM_new)
    for c_new in C_new:
        c_new_checked = True
        for c_old in C_final:
            if set(c_old[0]).issubset(c_new[0]):
                C_final.remove(c_old)
            if set(c_new[0]).issubset(c_old[0]):
                c_new_checked = False
        if (c_new not in C_final) & (c_new_checked):
            C_final.append(c_new)
    print(n, R)

C_final = sorted(C_final, key=lambda x: x[1])

len(C_final)

summary = pd.DataFrame()
summary['bg_id'] = list(zip(*C_final))[0]
summary['lemma'] = summary.bg_id.map(
    lambda x: tuple([id2lemma[l][0] for l in x]))
summary['reading'] = summary.bg_id.map(
    lambda x: tuple([id2lemma[l][1] for l in x]))
summary['average_similarity'] = list(zip(*C_final))[1]

summary
"""# Test"""

gamma = 0.6
k = 50
alpha = 0.2
beta = 0.8
gamma = 0.6

TOP = 500

token_lst = []
for poem in hd.cleaned:
    token_lst += poem

lemma_freq_dic = dict(OrderedDict(Counter(token_lst).most_common()))
target_lemma_lst = nlargest(TOP, lemma_freq_dic, key=lemma_freq_dic.get)
R = target_lemma_lst

# initial matrice
FreqM = co_oc_mat(hd.source.str.split(','), window_size)
ppmiM = ppmi(FreqM)
CosM = sim_mat(ppmiM, target_lemma_lst)

CosM = sim_mat(ppmiM, target_lemma_lst)

R = target_lemma_lst[:500]
R = ['BG-02-1521-05-0101']
# CosM = CosM.loc[R,R]

cluster(CosM.loc[R, R], R[0], k, alpha)

C = committee(R, CosM)
print(len(C))
C = filer_sort(C, CosM)
len(C)

avg_sim(CosM, ('BG-02-3394-07-0100', 'BG-01-5151-01-0100'))

CosM.loc['BG-02-3394-07-0100', 'BG-01-5151-01-0100']

for x in C:
    for l in x[0]:
        l = id2lemma[l][0]
        print(l)
    print(x[1])

CFreqM, CppmiM, ComM = merge_1(C, FreqM)

CppmiM

new_C_s = merge_2(ComM, CosM)

len(new_C_s)

for x in new_C_s:
    for l in x[0]:
        l = id2lemma[l][0]
        print(l)
    print(x[1])

len(R)
C_new = sorted(new_C_s, key=lambda x: x[1])
CFreqM_new, CppmiM_new, ComM_new = merge_1(C_new, FreqM)
len(residuals(R, CppmiM_new))

CFreqM_new

print(len(R))
print(R)
CosM = CosM.loc[R, R]
C = committee(R)
C = filer_sort(C)
print(C)
CFreqM, CppmiM, ComM = merge_1(C, FreqM)
c_lst = ComM.columns
new_C_s = merge_2(c_lst)
C_new = sorted(new_C_s, key=lambda x: x[1])
CfreqM_new, CppmiM_new, ComM_new = merge_1(C_new, FreqM)
R = residuals(R, CppmiM_new)
print(C_new)
